# main.py

from pipeline.rag_pipeline import RAGPipeline
from strategies.fetchers.bcb_fetcher import BCBFetcher
from strategies.splitters.default_splitter import DefaultSplitter
from strategies.embedders.openai_embedder import OpenAIEmbedder
from strategies.vectorstores.faiss_store import FaissStore
from strategies.llms.openai_llm import OpenAILLM
from utils.logger import get_logger
from utils.memory import ManualMemory

import os

logger = get_logger("main")


def build_index(pipeline):
    logger.info("üì• Coletando dados e construindo √≠ndice...")
    raw_data = pipeline.fetcher.fetch()
    chunks = pipeline.splitter.split(raw_data)
    vectors = pipeline.embedder.embed_documents(chunks)
    pipeline.vectorstore.build(vectors, chunks)
    pipeline.vectorstore.save()
    logger.info("‚úÖ √çndice constru√≠do e salvo.")


def main():
    logger.info("Iniciando execu√ß√£o do insights-rag-financas...")

    memory = ManualMemory()

    fetcher = BCBFetcher(
        series_id=11, data_inicial="01/01/2020", data_final="31/12/2024"
    )  # SELIC Brasil
    splitter = DefaultSplitter()
    embedder = OpenAIEmbedder()
    vectorstore = FaissStore()
    llm = OpenAILLM()

    pipeline = RAGPipeline(
        fetcher=fetcher,
        splitter=splitter,
        embedder=embedder,
        vectorstore=vectorstore,
        llm=llm,
    )

    index_path = vectorstore.index_path

    # Se √≠ndice n√£o existe, cria um
    if not os.path.exists(index_path):
        print("√çndice local n√£o encontrado. Construindo √≠ndice...")
        build_index(pipeline)
    else:
        print("√çndice local encontrado. Carregando √≠ndice...")
        pipeline.vectorstore.load()

    print(
        "\nü§ñ Bem-vindo ao Insights RAG Finan√ßas! Fa√ßa suas perguntas sobre a taxa SELIC no Brasil."
    )
    print("Comandos dispon√≠veis: 'sair', 'atualizar', 'resetar'\n")

    while True:
        question = input("‚ùì Sua pergunta: ").strip()
        if question.lower() in {"sair", "exit", "quit"}:
            print("üëã Encerrando o programa. At√© mais!")
            break
        if question.lower() == "atualizar":
            print("‚ôªÔ∏è Atualizando dados e reconstruindo √≠ndice, aguarde...")
            try:
                build_index(pipeline)
                print("‚úÖ √çndice atualizado com sucesso!\n")
            except Exception as e:
                logger.error(f"Erro ao atualizar √≠ndice: {e}")
                print("‚ö†Ô∏è Falha ao atualizar √≠ndice. Tente novamente mais tarde.\n")
            continue
        if question.lower() == "resetar":
            memory.reset()
            print("üßΩ Mem√≥ria da conversa apagada com sucesso!\n")
            continue
        if not question:
            print("‚ö†Ô∏è Pergunta vazia. Por favor, digite algo.")
            continue

        try:
            logger.info(f"Pergunta recebida: {question}")
            query_embedding = pipeline.embedder.embed_query(question)
            docs = pipeline.vectorstore.similarity_search(query_embedding)
            context = " ".join([doc.page_content for doc in docs])

            summary = memory.get_summary()
            context_with_memory = f"Resumo da conversa at√© agora:\n{summary or '[sem hist√≥rico]'}\n\n{context}"

            answer = pipeline.llm.answer(question, context_with_memory)
            print("\nüß† Resposta:", answer, "\n")
            memory.update_summary(question, answer)

        except Exception as e:
            logger.error(f"Erro durante o processamento: {e}")
            print(
                "‚ö†Ô∏è Ocorreu um erro ao processar sua pergunta. Por favor, tente novamente.\n"
            )


if __name__ == "__main__":
    main()
